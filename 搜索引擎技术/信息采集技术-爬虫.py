#!/usr/bin/python3
# -*- coding: utf-8 -*-
# @Time    : 2019/3/24 11:31
# @Author  : ylh
# @FileName: 信息采集技术-爬虫.py
# @Software: PyCharm
import requests
import re
a = requests.session()
url_index='https://finance.sina.com.cn/'
b = a.get(url=url_index,)
c = b.text
# print(c)
r = re.findall(re.compile('"(http:.*?)"'),c)
# r = re.findall(re.compile(r'<a.*?href=(.*?)"'),c)
print('kaishi ')
print(r)
r.remove('http://')
print(r)
r.append('https://finance.sina.com.cn/')
print(len(r))
sum_url_pagerank=[0]*783
'''  pagerank 简单矩阵
for i in r:
    try:
        content_2=requests.get(url=i).text
        r_2 = re.findall(re.compile('"(http:.*?)"'), content_2)
        for j in r_2:
            for x in range(len(r)):
                if r[x] ==j:
                    sum_url_pagerank[x] += 1
    except:
        pass
'''
# print(sum_url_pagerank)
number = [7, 417, 664, 21, 21, 23, 1064, 5, 117, 5, 2, 3, 23, 196, 23, 130, 152, 161, 197, 179, 171, 122, 84, 56, 59, 58, 9, 138, 122, 107, 10, 44, 7, 184, 26, 111, 171, 7, 26, 4, 68, 2, 21, 26, 2, 2, 2, 21, 19, 2, 2, 111, 15, 13, 11, 11, 34, 6, 15, 13, 11, 11, 34, 11, 21, 13, 5, 3, 8, 11, 11, 8, 11, 8, 6, 2, 11, 26, 75, 128, 52, 46, 47, 128, 52, 46, 47, 128, 52, 46, 47, 128, 52, 46, 47, 128, 52, 46, 47, 128, 52, 33, 52, 22, 33, 75, 33, 52, 22, 33, 75, 33, 52, 56, 33, 75, 71, 14, 47, 36, 80, 11, 2, 32, 80, 11, 18, 24, 75, 97, 31, 59, 101, 16, 12, 196, 161, 197, 179, 171, 12, 19, 11, 20, 30, 7, 83, 83, 19, 34, 3, 13, 24, 2, 24, 19, 8, 26, 26, 74, 7, 80, 18, 11, 35, 12, 37, 2, 21, 6, 12, 48, 4, 29, 35, 18, 26, 3, 37, 39, 42, 34, 59, 20, 8, 3, 3, 21, 44, 16, 24, 75, 97, 13, 35, 31, 5, 39, 36, 24, 71, 19, 11, 18, 35, 10, 3, 3, 43, 3, 35, 4, 18, 25, 16, 71, 68, 68, 197, 179, 171, 122, 50, 171, 197, 52, 179, 36, 122, 83, 3, 50, 50, 11, 184, 196, 18, 16, 17, 9, 20, 11, 196, 14, 130, 161, 152, 12, 2, 12, 36, 13, 9, 9, 138, 11, 138, 43, 138, 138, 138, 12, 46, 11, 42, 11, 43, 12, 44, 11, 47, 18, 47, 14, 32, 21, 14, 16, 28, 16, 22, 2, 31, 25, 2, 21, 2, 21, 2, 122, 13, 2, 13, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 8, 4, 84, 3, 111, 146, 2, 19, 25, 19, 9, 3, 13, 5, 13, 2, 2, 2, 146, 2, 6, 6, 6, 41, 4, 4, 4, 29, 2, 2, 2, 122, 2, 8, 10, 2, 2, 2, 2, 28, 2, 8, 2, 459, 8, 24, 5, 24, 2, 12, 13, 117, 41, 41, 2, 146, 9, 9, 7, 7, 7, 6, 44, 2, 2, 23, 7, 11, 9, 8, 2, 2, 2, 2, 2, 2, 2, 2, 11, 13, 13, 24, 24, 12, 12, 11, 11, 24, 24, 30, 12, 12, 12, 12, 12, 12, 6, 6, 6, 6, 6, 6, 12, 12, 12, 6, 6, 6, 15, 15, 15, 15, 15, 15, 13, 13, 15, 15, 15, 15, 15, 15, 15, 13, 15, 15, 83, 83, 83, 2, 196, 161, 23, 23, 39, 39, 26, 26, 26, 26, 26, 26, 26, 37, 26, 20, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 10, 10, 14, 14, 25, 25, 50, 50, 122, 122, 24, 24, 24, 24, 22, 22, 22, 22, 24, 24, 22, 22, 22, 22, 22, 22, 12, 12, 9, 9, 22, 9, 22, 16, 22, 22, 40, 22, 4, 2, 4, 4, 15, 15, 27, 12, 8, 15, 8, 2, 28, 8, 2, 16, 26, 8, 16, 32, 8, 2, 12, 8, 9, 74, 2, 2, 12, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 84, 56, 56, 59, 59, 84, 84, 179, 179, 171, 122, 171, 197, 36, 197, 111, 28, 28, 10, 10, 10, 10, 28, 10, 10, 197, 13, 16, 4, 4, 6, 7, 20, 8, 6, 6, 6, 8, 6, 4, 4, 2, 4, 113, 8, 2, 30, 30, 30, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 16, 16, 22, 22, 22, 30, 30, 12, 12, 12, 12, 12, 12, 32, 32, 14, 14, 113, 113, 14, 37, 17, 14, 21, 26, 14, 14, 113, 26, 8, 2, 2, 2, 12, 2, 5, 122, 15, 20, 24, 18, 16, 17, 11, 14, 8, 17, 5, 9, 44, 23, 23, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 11, 2, 44, 44, 4, 23, 21, 9, 7, 7, 7, 44, 3, 2, 3, 3, 2, 196, 130, 152, 161, 2, 8, 50, 197, 179, 171, 122, 12, 4, 2, 58, 56, 59, 11, 25, 11, 13, 24, 12, 138, 113, 12, 23, 44, 7, 7, 8, 13, 3, 107, 4, 111, 56, 34, 68, 2, 12, 2, 83, 40, 21, 9, 128, 75, 7, 10, 2, 3, 2, 9, 28, 17, 11, 2, 9, 11, 16, 20, 17, 9, 15, 3, 2, 7, 9, 2, 4, 5, 3, 2, 2, 2, 2, 4, 2, 431, 214, 50, 630, 631, 610, 578, 637, 638, 592, 564, 52, 2, 2, 2, 101, 409, 409, 0]
print(len(number))
final_arr= dict(zip(r,number))
print(final_arr)
final_arr_12=sorted(final_arr)
print(final_arr_12)
print(list(reversed(final_arr_12)))
# print(sorted(sum_url_pagerank))
###----简单搜索----###
'''
shu_ru = input('请输入搜索关键词：')
# print(b.content.decode('utf8'))
substence_index = b.content.decode('utf8')
if substence_index.index(shu_ru) != -1:
    print(url_index)
'''
# for i in r:
#     try:
#         substence = a.get(url= i).content.decode('utf8')
#         if substence.index(shu_ru)!=-1:
#             print(i)
#     except:
#         pass
# print(c.index(a))
'''百度搜索测试
# search = '斗罗大陆'
# demo='https://www.baidu.com/s?wd='
# b = a.get(url=demo+search,header='{User-Agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:54.0) Gecko/20100101 Firefox/54.0}')
# print(b.text)
'''
